%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald ArseneauEEEtran

% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.


\title{Artificial Intelligence in Cultural Heritage: Opportunities, Challenges, and Ethical Considerations}

\author{Ling Mianni  ~\IEEEmembership{}%
\thanks{Ling Mianni  Student ID: 24065622G. Email: [your.email@example.com]}}

\begin{document}

\maketitle

\begin{abstract}
Artificial Intelligence (AI) has emerged as a transformative tool in the preservation, analysis, and dissemination of cultural heritage. This paper explores the multifaceted applications of AI in cultural contexts, highlighting both the opportunities it presents and the ethical challenges it poses. We examine case studies where AI has been instrumental in restoring artifacts, translating ancient texts, and enhancing accessibility to cultural resources. Concurrently, we delve into concerns regarding cultural sensitivity, potential biases in AI algorithms, and the epistemological implications of machine-mediated interpretations. Through a comprehensive analysis, this study aims to provide a balanced perspective on integrating AI into cultural heritage initiatives, ensuring that technological advancements align with ethical standards and respect for cultural diversity.
\end{abstract}



% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers


% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.







% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{T}{he} integration of Artificial Intelligence (AI) into the humanities marks a transformative moment in both technological advancement and cultural reflection. Traditionally grounded in critical thinking, historical inquiry, and aesthetic appreciation, the humanities are now engaging with machine learning models, natural language processors, and generative systems capable of simulating human creativity and reasoning. This convergence prompts a fundamental question: how can artificial intelligence contribute meaningfully to the understanding, preservation, and reinterpretation of human culture?

As digital archives grow and multimodal datasets become available, AI technologies are increasingly employed to automate tasks that were once the domain of expert historians, archivists, or art conservators. Applications range from the classification of ancient manuscripts using convolutional neural networks (CNNs)~\cite{girbacia2024analysis} to the generation of embroidered cultural artifacts via region-aware generative adversarial networks (GANs)~\cite{hu2024msembgan}. The shift from manual to algorithmic approaches offers both opportunity and concern: while AI can enable unprecedented access and scalability, it may also introduce epistemic blind spots and ethical dilemmas.

This review surveys recent developments at the intersection of AI and the humanities, aiming to map current capabilities, challenges, and implications. We categorize the literature into four primary application areas:
\begin{itemize}
    \item AI for tangible heritage conservation, including visual inspection and structural health monitoring,
    \item AI for intangible heritage modeling, such as performing arts, language, and embroidery synthesis,
    \item AI for text and narrative analysis, covering literary classification, genre modeling, and digital storytelling,
    \item Ethical and social implications of AI in cultural heritage, including algorithmic bias, interpretability, and preservation authenticity.
\end{itemize}

We draw on ten selected papers published between 2022 and 2025, reflecting global perspectives, interdisciplinary collaboration, and methodological innovation. Through this synthesis, we assess both the potential and the pitfalls of AI-enhanced humanistic inquiry.


\section{Background and Methodology}

Artificial Intelligence has emerged as a key technological enabler across disciplines, yet its role in the humanities has only recently garnered scholarly and institutional attention. The adoption of AI in humanistic contexts is driven by a combination of technological innovation, data availability, and evolving academic paradigms that encourage computational approaches to cultural artifacts.

\subsection{The Rise of AI in the Humanities}

The digital humanities movement, which began with digitization and basic computational text analysis, has gradually matured into a space where AI techniques such as natural language processing (NLP), computer vision, and neural networks are used to analyze, preserve, and even generate cultural content. Scholars are now turning to AI to uncover patterns in literary corpora, simulate restoration procedures for ancient artifacts, and model the evolution of intangible heritage such as music and dance.

While early applications emphasized automation and accessibility, recent developments emphasize interpretability, context sensitivity, and cultural specificity. Projects such as the automatic classification of historical documents~\cite{girbacia2024analysis}, or the restoration of murals using predictive modeling at the Dunhuang Research Academy, exemplify the shift toward more nuanced AI-assisted scholarship.

\subsection{Methodological Approach}

This paper employs a literature review methodology, synthesizing key insights from ten recent studies between 2022 and 2025 that explore AI's application in the cultural and humanistic fields. These studies were selected based on their contribution to diverse AI methods (e.g., CNNs, LSTMs, GANs), relevance to heritage preservation (both tangible and intangible), and their engagement with interdisciplinary methodologies.

To structure our review, we categorize the literature into thematic clusters based on application domains. Each cluster is analyzed in terms of:
\begin{itemize}
    \item AI methods and algorithms employed,
    \item Cultural or heritage domain of application,
    \item Reported outcomes and performance metrics,
    \item Methodological or ethical challenges highlighted.
\end{itemize}

This systematic approach enables a comparative view across fields, revealing not only current capabilities but also limitations and unresolved tensions in AI–humanities integration.

\section{AI for Tangible Cultural Heritage}

AI technologies have significantly enhanced the documentation, analysis, and conservation of tangible cultural heritage—physical artifacts such as monuments, manuscripts, paintings, and architecture. This section reviews AI-based approaches for heritage diagnosis, modeling, and restoration.

\subsection{Visual Inspection and Structural Health Monitoring}

Mansuri et al.~\cite{mansuri2022heritage} outline a four-cluster framework for the application of AI in tangible heritage conservation, focusing particularly on visual inspection and structural health monitoring (SHM). Their study presents a case of an automatic visual inspection system developed for heritage structures, using deep learning models to detect damage types like cracks, spalling, and discoloration. These AI tools outperform manual methods by offering scalability, consistency, and reduced operational costs.

Similarly, the use of drone-collected imagery combined with YOLO-based algorithms for archaeological site monitoring demonstrates AI’s ability to autonomously detect degradation patterns, facilitating timely intervention.

\subsection{Digital Modeling and Restoration}

AI-powered digital modeling, especially through convolutional neural networks (CNNs) and generative adversarial networks (GANs), has become an essential tool for restoration and visualization. For instance, the work conducted at Dunhuang Academy employed historical image data to predict pigment loss in ancient murals with an accuracy of 89.3\%, leveraging deep learning-based image comparison and forecasting techniques~\cite{heritage2025aidecision}.

The integration of multispectral imaging and LiDAR with AI models allows for the generation of digital twins—3D replicas of artifacts—that can be used for restoration planning, public exhibition, and educational applications. These methods preserve structural and visual integrity while enabling access to fragile or remote artifacts.

\subsection{Challenges and Considerations}

While the technical benefits are substantial, challenges remain. Many heritage institutions lack digitized datasets of sufficient scale and quality, limiting the training of AI models. There is also an ongoing debate over the use of non-traditional restoration materials proposed by AI—for example, substituting fish glue with epoxy resin in Ming porcelain repairs~\cite{heritage2025aidecision}. Such decisions raise ethical questions about authenticity and long-term reversibility.

Furthermore, initiatives like the European Commission's 2023 \emph{AI Ethics Guidelines for Cultural Heritage} call for explainable AI methods. Techniques such as SHAP (Shapley Additive Explanations) are being introduced to visualize AI decision-making processes in restoration suggestions.


\section{AI for Intangible Cultural Heritage}

Intangible cultural heritage (ICH), including performing arts, oral traditions, rituals, traditional sports, and craftsmanship, presents unique challenges for documentation and preservation due to its non-physical and often context-sensitive nature. Recent AI advances—especially in deep learning, computer vision, and signal processing—are increasingly applied to capture, classify, and revitalize ICH in innovative ways.

\subsection{Performing Arts and Music Classification}

Chen et al.~\cite{chen2022cogcnet} propose a novel AI framework, CoGCNet, for the classification of Cantonese opera singing genres. By integrating Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks, their model captures both shallow acoustic features and deeper contextual dependencies between opera segments. The system achieved a classification accuracy of 95.6\% on a custom-built dataset of 1000 audio samples, demonstrating the potential for AI to model nuanced stylistic elements of traditional music.

Importantly, the authors emphasize the use of Mel-Frequency Cepstral Coefficients (MFCCs) and advanced spectrum analysis techniques to better align machine perception with human auditory interpretation. Their work contributes not only to heritage preservation but also to musicology and digital performance studies.

\subsection{Traditional Sports and Ritual Practices}

Wang~\cite{wang2024sports} and Xiao~\cite{xiao2024vision} explore AI applications in traditional Chinese martial arts and ritual sports. These studies demonstrate how 3D vision and motion capture combined with AI modeling can create intelligent teaching and preservation platforms. For example, using 3D reconstruction and multimodal AI, martial arts movements can be digitized, analyzed, and rendered in virtual environments, promoting embodied learning and intergenerational transmission.

Such AI-powered platforms support gesture recognition, movement standardization, and cross-linguistic accessibility via translation tools. Moreover, they provide immersive experiences that foster cross-cultural appreciation of ICH.

\subsection{Crafts, Textiles, and Generative Design}

Hu et al.~\cite{hu2024msembgan} introduce MSEmbGAN, a generative adversarial network capable of synthesizing diverse embroidery stitch types based on region-aware texture analysis. The model detects regions of an image and generates appropriate stitch patterns, preserving the style of traditional embroidery while enabling creative augmentation. The authors also released the largest publicly available embroidery dataset, supporting future AI research in textile heritage.

This approach exemplifies how generative AI can bridge traditional craftsmanship and modern design, facilitating the revitalization of ICH in fashion, gaming, and art installations. However, it also raises questions about authorship, cultural ownership, and the limits of automation in creative expression.

\section{Ethical and Epistemological Challenges}

While Artificial Intelligence (AI) offers transformative possibilities in the humanities, it simultaneously poses significant ethical, philosophical, and epistemological concerns. These challenges are not secondary technicalities—they are central to whether AI serves as a tool of cultural empowerment or a vehicle for erasure, reductionism, or digital colonialism. This section explores these tensions in four interrelated domains.

\subsection{Cultural Sensitivity and Representation}

AI systems deployed in cultural contexts must navigate the deep heterogeneity and complexity of human traditions. Yet many models are trained predominantly on Western, digitized, and canonical datasets, thereby reinforcing historical power asymmetries. For example, deep learning systems for music classification may perform well on European classical compositions but struggle with underrepresented oral genres, minority dialects, or hybrid contemporary forms~\cite{munster2024ai}.

This cultural imbalance leads to multiple ethical concerns:

\begin{itemize}
    \item \textbf{Erasure of Minoritized Cultures}: Traditions lacking digitized archives are excluded from training data, leading to algorithmic invisibility.
    \item \textbf{Contextual Loss}: Rituals, gestures, or texts are often stripped of their sociocultural contexts and interpreted solely as data patterns.
    \item \textbf{Misappropriation}: Algorithms may recombine cultural artifacts in aesthetically plausible but anthropologically problematic ways, risking appropriation or distortion.
\end{itemize}

To mitigate these risks, researchers advocate for \textbf{community-participatory AI} in which cultural stakeholders participate not just as data sources, but as co-designers and interpreters of AI systems. This shift from extractive to collaborative AI development is essential for respecting cultural sovereignty and preserving epistemic diversity.

\subsection{Algorithmic Fairness and Global Inequality}

AI’s potential to democratize heritage preservation is undermined by the uneven distribution of resources. Digitization infrastructures, machine learning expertise, and cloud processing power are concentrated in a few technologically advanced nations and institutions. As a result, many cultural heritage organizations in the Global South are unable to participate in or benefit from AI-enhanced initiatives.

This raises several pressing issues:

\begin{itemize}
    \item \textbf{Digital Colonialism}: When cultural data is harvested from under-resourced regions and processed by external AI systems without equitable returns or interpretive agency.
    \item \textbf{Loss of Local Epistemologies}: The dominance of computational logics risks overriding or marginalizing non-Western ways of knowing, such as oral history, communal authorship, or cyclical temporality.
    \item \textbf{Data Extractivism}: Digitized cultural materials can be monetized by commercial platforms without proper licensing, attribution, or benefit-sharing agreements.
\end{itemize}

To address these asymmetries, policies must support infrastructural investment in digital humanities across diverse geographies and ensure adherence to \textbf{FAIR (Findable, Accessible, Interoperable, Reusable)} and \textbf{CARE (Collective Benefit, Authority to Control, Responsibility, Ethics)} data principles.

\subsection{Explainability, Accountability, and Human Oversight}

One of the most widely discussed concerns in cultural AI applications is the opacity of model decision-making. In tasks such as automatic restoration, artifact classification, or visitor personalization, the rationale behind an AI’s output is often difficult to trace. This “black box” nature is especially dangerous in heritage contexts, where interpretive authority carries historical, ethical, and scholarly weight.

Explainable AI (XAI) offers technical tools such as:

\begin{itemize}
    \item \textbf{SHAP (Shapley Additive Explanations)}: Quantifies the contribution of each input feature to a model’s prediction.
    \item \textbf{LIME (Local Interpretable Model-Agnostic Explanations)}: Provides understandable local approximations of complex model behavior.
    \item \textbf{Saliency Maps}: Highlights regions in input images that most influenced a CNN’s classification.
\end{itemize}

However, technical explainability must be paired with \textbf{institutional accountability}. Cultural organizations should maintain \emph{human-in-the-loop} systems where expert review is required before AI-generated outputs are enacted. For instance, when an AI system recommends chemical treatments for ancient bronze artifacts, the recommendation must be reviewed for material compatibility, historical fidelity, and ethical appropriateness by conservators and historians~\cite{heritage2025aidecision}.

\subsection{Epistemological Tensions in Interpretation and Authorship}

AI systems do not interpret meaning in the way humanists do. They detect correlations, cluster patterns, and predict probable continuations. But interpretation in the humanities often relies on ambiguity, metaphor, contradiction, affect, and philosophical depth—dimensions poorly captured by algorithmic logic.

This raises critical questions:

\begin{itemize}
    \item \textbf{Can AI truly “read” a poem or merely mimic its structure?}
    \item \textbf{Do topic models and embeddings capture thematic resonance, or only surface co-occurrence?}
    \item \textbf{When AI generates a visual restoration or a translated version of a historical text, who is the author?}
\end{itemize}

Scholars such as Johanna Drucker argue that computational approaches risk \textbf{flattening interpretation}—substituting close reading with distant statistics. Others point to the risk of \textbf{hermeneutic overreach}, where human analysts mistakenly attribute meaning to outputs generated by statistical noise.

Thus, a key epistemological imperative is to \textbf{reframe AI as a heuristic}, not a final interpreter. AI can surface new questions, highlight patterns, or simulate alternatives—but the task of hermeneutics, critique, and meaning-making remains fundamentally human.

\subsection{Toward Responsible AI-Humanities Collaboration}

In light of these ethical and epistemological tensions, the way forward must include not just technical refinement but also cultural reflexivity. Key principles include:

\begin{itemize}
    \item \textbf{Reflexive Design}: Building AI systems that foreground ambiguity, pluralism, and layered meanings.
    \item \textbf{Ethics by Design}: Embedding fairness, transparency, and consent mechanisms at every stage of system development.
    \item \textbf{Epistemic Humility}: Recognizing the limits of what AI can (and cannot) know or represent in cultural contexts.
    \item \textbf{Dialogical Practice}: Encouraging dialogue between technologists, humanists, artists, and cultural communities to co-create AI tools that respect both computational and cultural logics.
\end{itemize}

In conclusion, the ethical and epistemological challenges of AI in the humanities are not merely constraints—they are the very terrain where responsible innovation must unfold. By confronting these tensions with rigor and humility, we can imagine an AI-infused humanities that is not reductive, but enriched; not homogenized, but plural; not mechanistic, but meaningfully human.


\section{Conclusion}

The intersection of Artificial Intelligence (AI) and the humanities marks a pivotal evolution in both domains. As this review has shown, AI has begun to reshape how we preserve, interpret, and interact with cultural heritage, from tangible artifacts to ephemeral oral traditions and intangible performances. This transformation is not merely technical—it is philosophical, institutional, and profoundly human.

\subsection{A Synthesis of Progress}

Across case studies and bibliometric analyses, a few dominant trends emerge. First, AI is facilitating large-scale digitization and classification of cultural assets at unprecedented speeds. Projects like CoGCNet for Cantonese opera genre recognition~\cite{chen2022cogcnet} and MSEmbGAN for embroidery synthesis~\cite{hu2024msembgan} reveal how convolutional neural networks and deep generative models are being customized for domain-specific tasks in heritage interpretation.

Second, cultural institutions are adopting AI for restoration and conservation. From automatic visual inspection systems for heritage buildings~\cite{mansuri2022visualinspection} to SVM- and random forest-based predictive models for material degradation~\cite{heritage2025aidecision}, machine learning is driving a shift from reactive to preventive conservation strategies.

Third, AI is enabling dynamic forms of engagement and reanimation. Virtual exhibitions, motion-based dance preservation, and smart interfaces for language translation are not just tools of display—they are reimagining how audiences learn, feel, and participate in cultural heritage.

\subsection{Risks and Responsibilities}

Despite these gains, the field must proceed with caution. AI, when poorly implemented, risks flattening cultural nuance into data points, or prioritizing algorithmic efficiency over lived experience. Concerns over bias, interpretability, and cultural appropriation are not peripheral—they are central.

For instance, if a neural network suggests the use of synthetic epoxies for porcelain restoration, contradicting centuries of artisanal methods~\cite{wang2024sports}, we must ask: Who decides the standard of authenticity? If a generative model simulates embroidery styles with remarkable fidelity, how do we distinguish between homage and reproduction? And when AI models are trained predominantly on artifacts from privileged regions or languages, do they inadvertently marginalize the heritage of less digitized cultures?

Ethical frameworks, such as the 2023 EU Guidelines on AI and Cultural Heritage, emphasize the importance of human oversight, community involvement, and transparency in algorithmic decisions. Explainable AI (XAI) techniques like SHAP values or attention maps are essential not only for academic rigor but for public accountability.

\subsection{Epistemological Futures}

Beyond ethics, there lies a deeper epistemological question: What does it mean for machines to "understand" culture? The humanities have traditionally thrived on ambiguity, metaphor, context, and critique. In contrast, AI systems thrive on pattern recognition and optimization. Reconciling these epistemologies is not trivial.

Some scholars argue for a "hybrid humanities," where machine learning augments rather than replaces human interpretation. In such models, AI can surface patterns or anomalies in large corpora, but the ultimate act of meaning-making remains human. Others envision a more radical rethinking, where cultural analytics, multimodal embeddings, and computational aesthetics become core tools of the humanist’s craft.

\subsection{Toward a Responsible AI-Humanities Convergence}

Moving forward, several principles should guide this convergence:

\begin{itemize}
    \item \textbf{Human-in-the-loop design}: Cultural heritage decisions should never be fully automated. AI must augment curatorial, archival, and scholarly judgment—not replace it.
    \item \textbf{Participatory modeling}: Communities whose heritage is being digitized must be involved in training data selection, labeling, and model interpretation.
    \item \textbf{Transparent methodologies}: Code, datasets, and modeling assumptions should be open to scrutiny by both technical and non-technical stakeholders.
    \item \textbf{Cultural pluralism}: Systems must support multilingual, multicultural, and multimodal representations of heritage, recognizing that no single format fits all.
    \item \textbf{Educational reform}: Humanities scholars need basic AI literacy, just as data scientists need cultural sensitivity. Interdisciplinary training programs will be vital.
\end{itemize}

\subsection{Final Reflections}

In the spirit of the humanities, we must ask not just what AI can do, but what it ought to do. The goal is not technological domination, but thoughtful collaboration. AI is not a neutral force—it reflects our values, priorities, and assumptions. Used responsibly, it can help safeguard endangered languages, animate forgotten traditions, and reveal hidden connections across time and space.

Ultimately, the promise of AI in the humanities is not about replacing human insight but amplifying it—opening new windows into the past while equipping us to tell richer, more inclusive stories for the future.







% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)








\begin{thebibliography}{99}

\bibitem{munster2024ai}
S.~Munster, ``AI, Cultural Heritage, and Bias: Some Key Queries That Arise from the Application of Machine Learning to Cultural Heritage,'' \emph{Heritage}, vol.~7, no.~11, pp. 287--299, 2024.

\bibitem{heritage2025aidecision}
F.~Di~Stefano and E.~S. Malinverni, ``Ethics of Artificial Intelligence for Cultural Heritage: Opportunities and Challenges,'' \emph{ISPRS Archives}, vol.~XLVIII-M-2-2023, pp. 1149--1156, 2023.

\bibitem{girbacia2024analysis}
A.~Girbacia, ``AI Integration in Cultural Heritage Conservation,'' \emph{International Journal of Educational and Developmental Innovation in Education}, vol.~2, no.~1, pp. 22--35, 2024.

\bibitem{colaner2022epistemology}
J.~Colaner, ``Connecting Ethics and Epistemology of AI,'' \emph{AI \& Society}, vol.~37, no.~1, pp. 123--135, 2022.

\bibitem{feri2025epistemic}
I.~Feri, ``Artificial Intelligence and the Epistemic Frontier: Reconfiguring Authority, Authorship, and Inquiry in Academia,'' \emph{ResearchGate Preprint}, 2025.

\bibitem{buolamwini2018gender}
J.~Buolamwini and T.~Gebru, ``Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,'' in \emph{Proceedings of the 1st Conference on Fairness, Accountability and Transparency}, 2018, pp. 77--91.

\bibitem{hayles2017unthought}
N.~K. Hayles, \emph{Unthought: The Power of the Cognitive Nonconscious}. University of Chicago Press, 2017.

\bibitem{he2024narratives}
Z.~He, J.~Su, L.~Chen, T.~Wang, and R.~LC, ``I Recall the Past: Exploring How People Collaborate with Generative AI to Create Cultural Heritage Narratives,'' \emph{arXiv preprint arXiv:2501.00359}, 2024.

\bibitem{bu2025misalignment}
F.~Bu, Z.~Wang, S.~Wang, and Z.~Liu, ``An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage,'' \emph{arXiv preprint arXiv:2501.02039}, 2025.

\bibitem{qadri2025thick}
R.~Qadri, M.~Diaz, D.~Wang, and M.~Madaio, ``The Case for 'Thick Evaluations' of Cultural Representation in AI,'' \emph{arXiv preprint arXiv:2503.19075}, 2025.

\bibitem{bayramli2025diffusion}
Z.~Bayramli et al., ``Diffusion Models Through a Global Lens: Are They Culturally Inclusive?,'' \emph{arXiv preprint arXiv:2502.08914}, 2025.

\bibitem{tabor2024bias}
F.~Tabor, ``AI and Cultural Representation: Addressing Bias for Equitable Outcomes,'' \emph{Francesca Tabor Blog}, 2024. [Online]. Available: \url{https://www.francescatabor.com/articles/2024/8/3/ai-and-cultural-representation-addressing-bias-for-equitable-outcomes}

\end{thebibliography}


% that's all folks
\end{document}


